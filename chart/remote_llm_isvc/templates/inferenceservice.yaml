apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    sidecar.istio.io/inject: "false"
  labels:
    access-kubeflow-minio: "true"
    notebook-proxy: "true"
    sidecar.istio.io/inject: "false"
  name: remote-llm
spec:
  predictor:
    containers:
      - name: remote-llm-container
        image: {{ .Values.container.image }}
        imagePullPolicy: Always
        resources:
          limits:
            cpu: {{ .Values.container.resources.limits.cpu }}
            memory: {{ .Values.container.resources.limits.memory }}
            {{- if .Values.container.resources.limits.nvidia_gpu }}
            nvidia.com/gpu: {{ .Values.container.resources.limits.nvidia_gpu }}
            {{- end }}
          requests:
            cpu: {{ .Values.container.resources.requests.cpu }}
            memory: {{ .Values.container.resources.requests.memory }}
            {{- if .Values.container.resources.requests.nvidia_gpu }}
            nvidia.com/gpu: {{ .Values.container.resources.requests.nvidia_gpu }}
            {{- end }}
        ports:
          - containerPort: 8080
        envFrom:
          - secretRef:
              name: remote-llm-secret
